---
description: Dispatch fast sub-agents for task execution and plan analysis; parallel batch execution when multiple unblocked tasks exist
alwaysApply: false
---

# Sub-Agent Dispatch

**Execution and planning both use sub-agents.** You MUST dispatch sub-agents when executing tasks from a plan — do not do the work yourself. **Concurrency is decided by Cursor:** feed it all runnable tasks that do not depend on each other (no file overlap); do not cap the batch size yourself. The orchestrator coordinates; sub-agents do bounded work with full context injected. **Dispatch** is the same prompt and workflow; choose the mechanism by what is available (see below).

## Task orchestration UI — TodoWrite protocol (MANDATORY)

Before dispatching **any** sub-agents for tg tasks, you MUST call **TodoWrite** to register the task list. This triggers Cursor's "Task orchestration for autonomous execution" panel. Cursor docs state that the agent sends multiple Task tool calls in a single message so subagents run simultaneously; one call per turn may not trigger the panel or parallel execution.

**Protocol:**

1. **Get tasks**: `pnpm tg next --plan "<Plan>" --json --limit 20`
2. **TodoWrite (merge=false)**: Map each task to a todo item: `{ id: taskId, content: title, status: "in_progress" }` for the current batch, `{ id: taskId, content: title, status: "pending" }` for later tasks. Call TodoWrite with this list before any dispatch.
3. **Dispatch**: When dispatching a batch of N runnable tasks, emit **N Task (or mcp_task) invocations in the SAME message/turn** — one call per task. Do not dispatch one task per turn; Cursor runs them in parallel and surfaces the orchestration UI when all batch calls are in one response.
4. **As each completes**: TodoWrite with `merge=true` to update that item to `"completed"`. On failure, update to `"cancelled"`.

**Single-task batch:** TodoWrite is still required. TodoWrite expects at least 2 items; when only one tg task is in the batch, add a second item (e.g. "Review & gate") so the list has 2 entries.

Keep `.cursor/agents/implementer.md` in context when starting the tg execution loop.

## Task prioritization (orchestrator)

`tg next` returns runnable tasks in a **default order**: plan priority (higher first), then task risk (low → high), then estimate (shorter first, nulls last), then created_at (oldest first). The lead may use the list as-is, or reorder by strategy (e.g. one plan first, quick wins first, or pick by vibes like a kanban). No mandatory reasoning; default order is a good default.

Agent templates live in `.cursor/agents/` (see README there). You build each sub-agent prompt by interpolating `tg context <taskId> --json` output into the template placeholders.

## Dispatch mechanisms (choose by environment)

- **In-IDE or terminal**: Use the Cursor Task tool (with `model="fast"`) or the **agent CLI** (`agent --model <fast-model> --print --trust ...`). See [docs/cursor-agent-cli.md](../docs/cursor-agent-cli.md) for CLI options and usage.
- **This environment** (when the Task tool or agent CLI is not available): Use **mcp_task** with the same built prompt and a short `description` (e.g. "Implement task: &lt;title&gt;" or "Planner analyst: gather context for plan"). Use `subagent_type` generalPurpose or explore as appropriate (e.g. explore for planner-analyst, generalPurpose for implementer/reviewer). The prompt and workflow are unchanged; only the invocation differs. Do not skip dispatch because the Task tool is not visible — use mcp_task.

## When to use sub-agents

- **Before creating a plan** — **MANDATORY.** You MUST dispatch the planner-analyst first. See Pattern 3 below. AGENT.md and plan-authoring.mdc both require this; skipping it is a critical failure.
- **When executing tasks from a plan** — **MANDATORY.** You MUST use Pattern 1 (parallel batch) or Pattern 2 (sequential). Dispatch implementer sub-agents; do not run tg start / code / tg done yourself. Feed all runnable, non-conflicting tasks; Cursor decides how many run in parallel. AGENT.md requires this; skipping it is a critical failure.
- **Well-scoped tasks** with clear intent and domain/skill docs — sub-agents follow the context.

Use **direct execution** (you do start → work → done yourself) only when: a sub-agent has failed twice and you are completing that task yourself, or the task is explicitly exploratory/ambiguous and not suitable for dispatch. Always log direct execution with `tg note <taskId> --msg "Direct execution: <reason>"`.

## Pattern 1: Parallel batch execution (primary) — MANDATORY for execution

When executing a plan with multiple unblocked tasks, you MUST use this pattern. **Feed all non-conflicting tasks; Cursor decides how many run in parallel.**

0. **TodoWrite**: Once you have the runnable batch (from steps 1–2), call TodoWrite with it (see Task orchestration UI — TodoWrite protocol above). Map each task to a todo item (id, content, status); use merge=false before any dispatch. For a single-task batch, add a second item (e.g. "Review & gate") so the list has at least 2 entries.
1. Run `pnpm tg next --plan "<Plan>" --json --limit 20` (or `tg next --json --limit 20` for multi-plan) to get runnable tasks.
2. **File conflict check**: From the list, keep only tasks that do not touch the same files (compare `file_tree`, `suggested_changes`, or intent). Do not cap the batch size — include every runnable task that is independent of the others. Cursor will decide how many sub-agents run concurrently.
3. For each task in the batch, run `pnpm tg context <taskId> --json` and capture the JSON.
4. Inspect the `agent` field from the context JSON: if set, select the corresponding agent template (`.cursor/agents/{{AGENT}}.md`); otherwise default to `.cursor/agents/implementer.md`. Build the sub-agent prompt by replacing placeholders (`{{TASK_ID}}`, `{{AGENT_NAME}}`, `{{TITLE}}`, `{{INTENT}}`, `{{DOC_PATHS}}`, `{{SKILL_DOCS}}`, `{{SUGGESTED_CHANGES}}`, `{{FILE_TREE}}`, `{{RISKS}}`, `{{RELATED_DONE}}`, `{{EXPLORER_OUTPUT}}`) with values from the context JSON. Refer to the agent registry rule at `.cursor/rules/available-agents.mdc` for valid agent names. Use unique agent names (e.g. implementer-1, implementer-2, …) so `tg status` shows distinct workers.
5. **Dispatch** all tasks in the batch **in the same response** (one Task or mcp_task call per task in the batch). Do not dispatch one task per turn — emit all N calls in the same message so Cursor runs them in parallel and surfaces the orchestration UI. Same built prompt and a short description (e.g. "Implement task: <title>") per call. Cursor decides concurrency; do not artificially limit the number.
6. Wait for all to complete. For each completed task, if you have the implementer's diff, run the **two-stage review**: (1) dispatch **spec-reviewer** (read `.cursor/agents/spec-reviewer.md`, build prompt with task context + diff; Task tool, agent CLI, or mcp_task). (2) If spec-reviewer returns PASS, dispatch **quality-reviewer** (read `.cursor/agents/quality-reviewer.md`, build prompt with task context + diff; same mechanism). (3) If either returns FAIL, re-dispatch the implementer with that reviewer's specific feedback. After 2 re-dispatch failures for that task, do that task yourself (direct execution).
7. **Follow-up from notes/evidence**: For each completed task, read the implementer's return message and any notes on the task (e.g. `tg show <taskId>` for recent events). If the sub-agent reported environment limitations, gate failures, or suggested follow-up, apply the **Follow-up from notes/evidence** protocol below.
8. Run `pnpm tg next --plan "<Plan>" --json --limit 20` again. If more runnable tasks exist, go to step 2. Otherwise continue with any remaining work or finish the plan.

## Pattern 2: Sequential single-task execution — MANDATORY when only one runnable

When only one task is runnable or tasks share files (so you cannot batch), you MUST still use a sub-agent:

0. **TodoWrite**: Call TodoWrite with the task list (see Task orchestration UI — TodoWrite protocol above). For a single task, use 2 items (the task plus e.g. "Review & gate") so the list has at least 2 entries; update statuses with merge=true as the task completes.
1. Run `pnpm tg next --plan "<Plan>" --limit 1` and pick the task.
2. Optionally dispatch the **explorer** (read `.cursor/agents/explorer.md`, build prompt; use Task tool, agent CLI, or mcp_task). Use explorer output to enrich implementer context. Skip explorer for very simple tasks (e.g. add a test file, document X).
3. Run `pnpm tg context <taskId> --json`. Build the implementer prompt from the template and context (and explorer output if present). **Dispatch** one implementer (Task tool, agent CLI, or mcp_task) with the built prompt and description "Implement: <title>".
4. When implementer completes, run the **two-stage review**: (1) dispatch **spec-reviewer** (read `.cursor/agents/spec-reviewer.md`, build prompt with task context + diff; Task tool, agent CLI, or mcp_task). (2) If spec-reviewer returns PASS, dispatch **quality-reviewer** (read `.cursor/agents/quality-reviewer.md`, build prompt with task context + diff; same mechanism). (3) If either returns FAIL, re-dispatch the implementer with that reviewer's specific feedback. After 2 re-dispatch failures, complete the task yourself.
5. **Follow-up from notes/evidence**: Read the implementer's return message and any notes on the task (`tg show <taskId>`). If they reported environment limitations, gate failures, or suggested follow-up, apply the **Follow-up from notes/evidence** protocol below.
6. Repeat from step 1 for the next task.

## Follow-up from notes/evidence (orchestrator decides)

When an implementer (or other sub-agent) completes and their return message, evidence, or task notes mention environment limitations (e.g. "bun not available"), gate failures (e.g. "typecheck failed"), or suggested follow-up (e.g. "fix bun-types in tsconfig"), the **orchestrator** must decide whether to investigate further.

1. **Evaluate**: Does the finding warrant a follow-up task? Consider: Is it blocking the plan or other work? A known limitation of the sub-agent environment that the human can ignore? Out of scope for the current plan?
2. **If follow-up is warranted**: Create one or more tasks with `tg task new "<title>" --plan <planId>` (use the plan of the task that just completed). Optionally record the link: `tg note <completedTaskId> --msg "Follow-up: <newTaskId> <brief reason>"`. Delegate the new task(s) via the normal dispatch flow (they will appear in the next `tg next` batch, or dispatch them in the same batch if you have capacity).
3. **If not warranted**: Continue without creating tasks. Optionally `tg note <completedTaskId> --msg "Orchestrator: no follow-up; <brief reason>"` for audit.

Implementers should leave a `tg note` when they hit environment or gate issues they cannot fix, so the orchestrator can evaluate and optionally spawn follow-up tasks.

## Pattern 3: Plan analysis (before writing a plan) — MANDATORY

**You must use this pattern whenever the user asks for a plan.** Do not write a plan without first dispatching the planner-analyst.

Before creating a new plan (e.g. user asked for a feature and you are about to write `plans/yy-mm-dd_<name>.md`):

1. Read `.cursor/agents/planner-analyst.md`. Build the prompt with `{{REQUEST}}` = user's feature/request. Optionally run `pnpm tg status` and pass the output so the analyst can reference current plans and done tasks.
2. **Dispatch** the planner-analyst (Task tool, agent CLI, or mcp_task with subagent_type explore) with the built prompt and description "Planner analyst: gather context for plan".
3. Use the analyst's output as input when you write the plan. **You own architecture, dependencies, and task design** — the analyst gathers facts, you do the reasoning.

### Orchestrator critique checklist (after receiving analyst output)

Do not transcribe the analyst's suggested breakdown into tasks verbatim. Before writing the plan, work through this checklist:

- **Existing data first**: What metrics or insights can be derived from data already in the system (timestamps, event counts, existing fields) before designing new data capture? The analyst should have surfaced this; if not, identify it yourself.
- **Dependency minimization**: For each proposed dependency between tasks, ask: "Can the downstream task work (even partially) without the upstream?" Decouple data-capture features from features that can function without that data. Prefer a wide graph over a deep chain.
- **Concrete metrics**: When the user asks for something qualitative ("efficiency", "performance", "improvements"), define it in measurable terms in the plan. What exactly gets measured? What thresholds or comparisons make it actionable? If the analyst left this vague, sharpen it.
- **Task specificity**: Each task must be concrete enough that a fast sub-agent can implement it without making design decisions. If intent says "heuristics e.g. ..." or "optionally ...", the task needs more design work from you before it's ready.
- **Resolve open questions**: Architectural choices (e.g. "same command or separate?") should be decided in the plan, not left as open questions for implementers. If you genuinely can't decide, create an explicit investigate task.
- **Test coverage**: Either include test expectations in each task's intent, or create a dedicated test task. Don't list tests in plan-level `tests` without assigning them to a task.

## Learning Mode

**Toggle**: Check `"learningMode"` in `.taskgraph/config.json`. If `true`, learning mode is ON — run the review protocol below after each sub-agent completes. If `false` or absent, skip this section entirely.

### When it triggers

After every **implementer**, **explorer**, or **planner-analyst** sub-agent completes. Not after the reviewer (its job is already evaluation).

### Review protocol (orchestrator only)

When learning mode is ON and a sub-agent has completed:

1. **Gather the sub-agent's output**: the returned message, and for implementers also the git diff (`git diff HEAD~1` or similar).
2. **Evaluate**: Would you (the orchestrator) have done the same thing? Consider:
   - **Scope**: Did the sub-agent stay within the task's intent, or drift?
   - **Approach**: Was the implementation approach what you'd have chosen? If not, what's better and why?
   - **Quality**: Missing error handling, unnecessary complexity, wrong abstractions, missed patterns from the codebase?
   - **Context use**: Did the sub-agent make good use of the domain docs, skill guides, and suggested changes it was given? Or did it ignore them?
3. **Decide**: Is there a constructive, reusable learning here? Skip if the sub-agent did fine — only write learnings when there's a concrete improvement to encode. Not every task produces a learning.
4. **Write the learning**: Append to the `## Learnings` section of the relevant agent file (e.g. `.cursor/agents/implementer.md`). Use this format:

```
- **[YYYY-MM-DD]** <one-line summary>. <concrete directive: "Instead, do X" or "Always check Y before Z".>
```

5. **Consolidate** (periodically): When `## Learnings` exceeds ~10 entries, fold recurring patterns into the agent's main prompt template and remove the individual entries. This keeps the learnings section high-signal and prevents prompt bloat.

### Injecting learnings into prompts

When building a sub-agent prompt, if the agent file has a non-empty `## Learnings` section, include it in the prompt as a `{{LEARNINGS}}` block:

```
**Learnings from prior runs (follow these):**
{{LEARNINGS}}
```

Place this after the main instructions and before any task-specific context. The sub-agent should treat learnings as corrections to its default behavior.

## Lifecycle and errors

- Every implementer sub-agent must run `pnpm tg start <taskId> --agent <name>` at the start and `pnpm tg done <taskId> --evidence "..."` at the end. Do not skip — the task graph is the source of truth.
- If a fast sub-agent fails (error, wrong result, or spec-reviewer/quality-reviewer FAIL): re-dispatch once with clearer context or that reviewer's feedback. If it fails again, complete that task yourself with direct execution (session model).
- When dispatching in parallel, ensure each task has a distinct agent name (`implementer-1`, etc.) so `tg status` and events stay clear.

## Building prompts from context JSON

`pnpm tg context <taskId> --json` returns an object with: `task_id`, `title`, `agent`, `docs`, `skills`, `change_type`, `suggested_changes`, `file_tree`, `risks`, `doc_paths`, `skill_docs`, `related_done_by_domain`, `related_done_by_skill`. Map these to the placeholders in each agent template (see `.cursor/agents/README.md`). For the spec-reviewer and quality-reviewer you also need the implementer's git diff (e.g. `git diff` or `git show` after the implementer's run).

Additionally, for every agent: read the agent file's `## Learnings` section (if non-empty) and inject it as `{{LEARNINGS}}`. See Learning Mode above.
