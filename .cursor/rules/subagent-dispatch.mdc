---
description: Dispatch fast sub-agents for task execution and plan analysis; parallel batch execution when multiple unblocked tasks exist
alwaysApply: false
---

# Sub-Agent Dispatch

**Execution and planning both use sub-agents.** You MUST dispatch sub-agents when executing tasks from a plan — do not do the work yourself. Max **5** tasks in flight at a time. The orchestrator coordinates; sub-agents do bounded work with full context injected. **Dispatch** is the same prompt and workflow; choose the mechanism by what is available (see below).

Agent templates live in `.cursor/agents/` (see README there). You build each sub-agent prompt by interpolating `tg context <taskId> --json` output into the template placeholders.

## Dispatch mechanisms (choose by environment)

- **In-IDE or terminal**: Use the Cursor Task tool (with `model="fast"`) or the **agent CLI** (`agent --model <fast-model> --print --trust ...`). See [docs/cursor-agent-cli.md](../docs/cursor-agent-cli.md) for CLI options and usage.
- **This environment** (when the Task tool or agent CLI is not available): Use **mcp_task** with the same built prompt and a short `description` (e.g. "Implement task: &lt;title&gt;" or "Planner analyst: gather context for plan"). Use `subagent_type` generalPurpose or explore as appropriate (e.g. explore for planner-analyst, generalPurpose for implementer/reviewer). The prompt and workflow are unchanged; only the invocation differs. Do not skip dispatch because the Task tool is not visible — use mcp_task.

## When to use sub-agents

- **Before creating a plan** — **MANDATORY.** You MUST dispatch the planner-analyst first. See Pattern 3 below. AGENT.md and plan-authoring.mdc both require this; skipping it is a critical failure.
- **When executing tasks from a plan** — **MANDATORY.** You MUST use Pattern 1 (parallel, max 3) or Pattern 2 (sequential). Dispatch implementer sub-agents; do not run tg start / code / tg done yourself. Max **3** concurrent tasks. AGENT.md requires this; skipping it is a critical failure.
- **Well-scoped tasks** with clear intent and domain/skill docs — sub-agents follow the context.

Use **direct execution** (you do start → work → done yourself) only when: a sub-agent has failed twice and you are completing that task yourself, or the task is explicitly exploratory/ambiguous and not suitable for dispatch. Always log direct execution with `tg note <taskId> --msg "Direct execution: <reason>"`.

## Pattern 1: Parallel batch execution (primary) — MANDATORY for execution

When executing a plan with multiple unblocked tasks, you MUST use this pattern. **Max 3 concurrent.**

1. Run `pnpm tg next --plan "<Plan>" --json --limit 5` to get up to 5 runnable tasks.
2. **File conflict check**: If tasks might touch the same files (e.g. same path in `file_tree` or `suggested_changes`), do not parallelize those together — run them sequentially or batch only independent ones.
3. For each task in the batch, run `pnpm tg context <taskId> --json` and capture the JSON.
4. Inspect the `agent` field from the context JSON: if set, select the corresponding agent template (`.cursor/agents/{{AGENT}}.md`); otherwise default to `.cursor/agents/implementer.md`. Build the sub-agent prompt by replacing placeholders (`{{TASK_ID}}`, `{{AGENT_NAME}}`, `{{TITLE}}`, `{{INTENT}}`, `{{DOC_PATHS}}`, `{{SKILL_DOCS}}`, `{{SUGGESTED_CHANGES}}`, `{{FILE_TREE}}`, `{{RISKS}}`, `{{RELATED_DONE}}`, `{{EXPLORER_OUTPUT}}`) with values from the context JSON. Refer to the agent registry rule at `.cursor/rules/available-agents.mdc` for valid agent names. Use unique agent names (e.g. implementer-1, explorer-1, reviewer-1) so `tg status` shows distinct workers.
5. **Dispatch** up to **5** sub-agents **concurrently**: use the Task tool, agent CLI, or mcp_task (see Dispatch mechanisms above). Same built prompt and a short description (e.g. "Implement task: <title>").
6. Wait for all to complete. For each completed task, if you have the implementer's diff, dispatch the **reviewer** (read `.cursor/agents/reviewer.md`, build prompt with task context + diff; use Task tool, agent CLI, or mcp_task). If reviewer returns FAIL, re-dispatch the implementer once with the feedback; after 2 failures, do that task yourself (direct execution).
7. Run `pnpm tg next --plan "<Plan>" --json --limit 5` again. If more runnable tasks exist, go to step 3. Otherwise continue with any remaining work or finish the plan.

## Pattern 2: Sequential single-task execution — MANDATORY when only one runnable

When only one task is runnable or tasks share files (so you cannot batch), you MUST still use a sub-agent:

1. Run `pnpm tg next --plan "<Plan>" --limit 1` and pick the task.
2. Optionally dispatch the **explorer** (read `.cursor/agents/explorer.md`, build prompt; use Task tool, agent CLI, or mcp_task). Use explorer output to enrich implementer context. Skip explorer for very simple tasks (e.g. add a test file, document X).
3. Run `pnpm tg context <taskId> --json`. Build the implementer prompt from the template and context (and explorer output if present). **Dispatch** one implementer (Task tool, agent CLI, or mcp_task) with the built prompt and description "Implement: <title>".
4. When implementer completes, dispatch the **reviewer** with task context + diff (same mechanism). If FAIL, re-dispatch implementer with feedback once; after 2 failures, complete the task yourself.
5. Repeat from step 1 for the next task.

## Pattern 3: Plan analysis (before writing a plan) — MANDATORY

**You must use this pattern whenever the user asks for a plan.** Do not write a plan without first dispatching the planner-analyst.

Before creating a new plan (e.g. user asked for a feature and you are about to write `plans/yy-mm-dd_<name>.md`):

1. Read `.cursor/agents/planner-analyst.md`. Build the prompt with `{{REQUEST}}` = user's feature/request. Optionally run `pnpm tg status` and pass the output so the analyst can reference current plans and done tasks.
2. **Dispatch** the planner-analyst (Task tool, agent CLI, or mcp_task with subagent_type explore) with the built prompt and description "Planner analyst: gather context for plan".
3. Use the analyst's output as input when you write the plan. **You own architecture, dependencies, and task design** — the analyst gathers facts, you do the reasoning.

### Orchestrator critique checklist (after receiving analyst output)

Do not transcribe the analyst's suggested breakdown into tasks verbatim. Before writing the plan, work through this checklist:

- **Existing data first**: What metrics or insights can be derived from data already in the system (timestamps, event counts, existing fields) before designing new data capture? The analyst should have surfaced this; if not, identify it yourself.
- **Dependency minimization**: For each proposed dependency between tasks, ask: "Can the downstream task work (even partially) without the upstream?" Decouple data-capture features from features that can function without that data. Prefer a wide graph over a deep chain.
- **Concrete metrics**: When the user asks for something qualitative ("efficiency", "performance", "improvements"), define it in measurable terms in the plan. What exactly gets measured? What thresholds or comparisons make it actionable? If the analyst left this vague, sharpen it.
- **Task specificity**: Each task must be concrete enough that a fast sub-agent can implement it without making design decisions. If intent says "heuristics e.g. ..." or "optionally ...", the task needs more design work from you before it's ready.
- **Resolve open questions**: Architectural choices (e.g. "same command or separate?") should be decided in the plan, not left as open questions for implementers. If you genuinely can't decide, create an explicit investigate task.
- **Test coverage**: Either include test expectations in each task's intent, or create a dedicated test task. Don't list tests in plan-level `tests` without assigning them to a task.

## Learning Mode

**Toggle**: Check `"learningMode"` in `.taskgraph/config.json`. If `true`, learning mode is ON — run the review protocol below after each sub-agent completes. If `false` or absent, skip this section entirely.

### When it triggers

After every **implementer**, **explorer**, or **planner-analyst** sub-agent completes. Not after the reviewer (its job is already evaluation).

### Review protocol (orchestrator only)

When learning mode is ON and a sub-agent has completed:

1. **Gather the sub-agent's output**: the returned message, and for implementers also the git diff (`git diff HEAD~1` or similar).
2. **Evaluate**: Would you (the orchestrator) have done the same thing? Consider:
   - **Scope**: Did the sub-agent stay within the task's intent, or drift?
   - **Approach**: Was the implementation approach what you'd have chosen? If not, what's better and why?
   - **Quality**: Missing error handling, unnecessary complexity, wrong abstractions, missed patterns from the codebase?
   - **Context use**: Did the sub-agent make good use of the domain docs, skill guides, and suggested changes it was given? Or did it ignore them?
3. **Decide**: Is there a constructive, reusable learning here? Skip if the sub-agent did fine — only write learnings when there's a concrete improvement to encode. Not every task produces a learning.
4. **Write the learning**: Append to the `## Learnings` section of the relevant agent file (e.g. `.cursor/agents/implementer.md`). Use this format:

```
- **[YYYY-MM-DD]** <one-line summary>. <concrete directive: "Instead, do X" or "Always check Y before Z".>
```

5. **Consolidate** (periodically): When `## Learnings` exceeds ~10 entries, fold recurring patterns into the agent's main prompt template and remove the individual entries. This keeps the learnings section high-signal and prevents prompt bloat.

### Injecting learnings into prompts

When building a sub-agent prompt, if the agent file has a non-empty `## Learnings` section, include it in the prompt as a `{{LEARNINGS}}` block:

```
**Learnings from prior runs (follow these):**
{{LEARNINGS}}
```

Place this after the main instructions and before any task-specific context. The sub-agent should treat learnings as corrections to its default behavior.

## Lifecycle and errors

- Every implementer sub-agent must run `pnpm tg start <taskId> --agent <name>` at the start and `pnpm tg done <taskId> --evidence "..."` at the end. Do not skip — the task graph is the source of truth.
- If a fast sub-agent fails (error, wrong result, or reviewer FAIL): re-dispatch once with clearer context or feedback. If it fails again, complete that task yourself with direct execution (session model).
- When dispatching in parallel, ensure each task has a distinct agent name (`implementer-1`, etc.) so `tg status` and events stay clear.

## Building prompts from context JSON

`pnpm tg context <taskId> --json` returns an object with: `task_id`, `title`, `agent`, `docs`, `skills`, `change_type`, `suggested_changes`, `file_tree`, `risks`, `doc_paths`, `skill_docs`, `related_done_by_domain`, `related_done_by_skill`. Map these to the placeholders in each agent template (see `.cursor/agents/README.md`). For the reviewer you also need the implementer's git diff (e.g. `git diff` or `git show` after the implementer's run).

Additionally, for every agent: read the agent file's `## Learnings` section (if non-empty) and inject it as `{{LEARNINGS}}`. See Learning Mode above.
