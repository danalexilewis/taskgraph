---
description: Dispatch fast sub-agents for task execution and plan analysis; parallel batch execution when multiple unblocked tasks exist
alwaysApply: false
---

# Sub-Agent Dispatch

**Execution and planning both use sub-agents.** You MUST dispatch sub-agents when executing tasks from a plan — do not do the work yourself. **Concurrency is decided by Cursor:** feed it all runnable tasks that do not depend on each other (no file overlap); do not cap the batch size yourself. The orchestrator coordinates; sub-agents do bounded work with full context injected. **Dispatch** is the same prompt and workflow; choose the mechanism by what is available (see below).

## Task orchestration UI — TodoWrite protocol (MANDATORY)

Before dispatching **any** sub-agents for tg tasks, you MUST call **TodoWrite** to register the task list. This triggers Cursor's "Task orchestration for autonomous execution" panel. Cursor docs state that the agent sends multiple Task tool calls in a single message so subagents run simultaneously; one call per turn may not trigger the panel or parallel execution.

**Protocol:**

1. **Get tasks**: `pnpm tg next --plan "<Plan>" --json --limit 20`
2. **TodoWrite (merge=false)**: Map each task to a todo item: `{ id: taskId, content: title, status: "in_progress" }` for the current batch, `{ id: taskId, content: title, status: "pending" }` for later tasks. Call TodoWrite with this list before any dispatch.
3. **Dispatch**: When dispatching a batch of N runnable tasks, emit **N Task (or mcp_task) invocations in the SAME message/turn** — one call per task. Do not dispatch one task per turn; Cursor runs them in parallel and surfaces the orchestration UI when all batch calls are in one response.
4. **As each completes**: TodoWrite with `merge=true` to update that item to `"completed"`. On failure, update to `"cancelled"`.

**Single-task batch:** TodoWrite is still required. TodoWrite expects at least 2 items; when only one tg task is in the batch, add a second item (e.g. "Review & gate") so the list has 2 entries.

Keep `.cursor/agents/implementer.md` in context when starting the tg execution loop.

## Task prioritization (orchestrator)

`tg next` returns runnable tasks in a **default order**: plan priority (higher first), then task risk (low → high), then estimate (shorter first, nulls last), then created_at (oldest first). So the list is already a suggested priority. The lead may:

- **Use the list as-is** — Treat order as priority; batch from the top (after file-conflict filter). No extra logic.
- **Reorder by strategy** — E.g. finish one plan before switching; or pick quick wins (low risk, small estimate) first; or prefer the plan with the most blocked tasks to unblock it. Same as a dev picking from a kanban by vibes.
- **Use the signals in JSON** — Each task has `risk`, `estimate_mins`, `plan_title`; use them to reason if you want (e.g. "batch all low-risk under 30 min first"), or ignore and pick by list order.

No mandatory reasoning logic. The default order is a good default; discretion is fine.

Agent templates live in `.cursor/agents/` (see README there). You build each sub-agent prompt by interpolating `tg context <taskId> --json` output into the template placeholders.

## Worktrunk — standard for sub-agent worktrees

**Sub-agent task isolation uses Worktrunk (wt) when available.** Ensure `.taskgraph/config.json` has `"useWorktrunk": true` for repos where orchestrators delegate to implementers (or rely on auto-detect: if `wt` is on PATH and useWorktrunk is unset, tg uses Worktrunk). Raw git worktrees remain supported when `useWorktrunk` is false or wt is not installed.

- **Orchestrator**: When dispatching implementers in parallel, use `tg start <taskId> --agent <name> --worktree` (from repo root) so each task gets an isolated worktree. The worktree path is stored in the started event and is visible via `tg worktree list --json`; pass **WORKTREE_PATH** (or equivalent placeholder) in the implementer prompt so the sub-agent runs all work and `tg done` from that directory.
- **Implementer**: Receives WORKTREE_PATH from the orchestrator or obtains it after running `tg start ... --worktree` (e.g. from `tg worktree list --json`). Must `cd` to that path before making changes and run `tg done` from there.

## Dispatch mechanisms (choose by environment)

- **In-IDE or terminal**: Use the Cursor Task tool (with `model="fast"`) or the **agent CLI** (`agent --model <fast-model> --print --trust ...`). See [docs/cursor-agent-cli.md](../docs/cursor-agent-cli.md) for CLI options and usage.
- **This environment** (when the Task tool or agent CLI is not available): Use **mcp_task** with the same built prompt and a short `description` (e.g. "Implement task: &lt;title&gt;" or "Planner analyst: gather context for plan"). Use `subagent_type` generalPurpose or explore as appropriate (e.g. explore for planner-analyst, generalPurpose for implementer/reviewer). The prompt and workflow are unchanged; only the invocation differs. Do not skip dispatch because the Task tool is not visible — use mcp_task.

## Test coverage / execution

- **Plan-end structure:** The last tasks of every plan must be **add-tests** (one or more tasks that create tests for new features) then **run-full-suite** (e.g. `pnpm gate:full`). These are dedicated tasks in the plan, blocked by all feature work. See plan-authoring.mdc and docs/plan-format.md.
- **Implementers do not run tests:** Implementer sub-agents do not run the test suite; they complete their task and report evidence (commands run, git ref). Tests are added and run only in the dedicated add-tests and run-full-suite tasks at plan end.
- **When run-full-suite fails:** Use the **Follow-up from notes/evidence** protocol below (lead-informed on failure): the run-full-suite task is marked done with failure in evidence and a `tg note`; the orchestrator creates fix tasks or escalates.

## When to use sub-agents

- **Before creating a plan** — **MANDATORY.** You MUST dispatch the planner-analyst first. See Pattern 3 below. AGENT.md and plan-authoring.mdc both require this; skipping it is a critical failure.
- **When executing tasks from a plan** — **MANDATORY.** You MUST use Pattern 1 (parallel batch) or Pattern 2 (sequential). Dispatch implementer sub-agents; do not run tg start / code / tg done yourself. Feed all runnable, non-conflicting tasks; Cursor decides how many run in parallel. AGENT.md requires this; skipping it is a critical failure.
- **Well-scoped tasks** with clear intent and domain/skill docs — sub-agents follow the context.

Use **direct execution** (you do start → work → done yourself) only when: a sub-agent has failed twice and you are completing that task yourself, or the task is explicitly exploratory/ambiguous and not suitable for dispatch. Always log direct execution with `tg note <taskId> --msg "Direct execution: <reason>"`.

## Escalation decision tree

Use this to decide **re-dispatch** vs **direct execution** vs **escalate to human**.

- **Re-dispatch (same or adjusted sub-agent)**
  - First failure: spec-reviewer or quality-reviewer returned FAIL — re-dispatch implementer once with that reviewer’s specific feedback.
  - Sub-agent error or wrong result with clear fix — re-dispatch once with clearer context or instructions.
  - Environment flake (e.g. transient timeout) — one retry is acceptable; then treat as second failure.

- **Direct execution (orchestrator does the task)**
  - Sub-agent has failed **twice** on the same task (two re-dispatch attempts) — complete the task yourself; log with `tg note <taskId> --msg "Direct execution: 2 sub-agent failures"`.
  - Task is **explicitly exploratory or ambiguous** and not suitable for a scoped sub-agent — do start → work → done yourself; log with `tg note <taskId> --msg "Direct execution: exploratory/ambiguous"`.

- **Architectural escalation** (before human escalation)
  - If the same area has failed across **3+ tasks** (not just retries on one task) — the problem may be architectural, not task-level. Create an **investigate** task targeting that area before creating more fix tasks. Log: `tg note <taskId> --msg "Architectural concern: 3+ failures in <area>"`.

- **Escalate to human**
  - **Credentials or secrets** required (API keys, auth, deploy tokens) — stop and ask the user to supply or approve; do not attempt to guess or inject.
  - **Ambiguous intent** that cannot be resolved from context or plan (e.g. conflicting requirements, missing product decision) — summarize and ask the user to clarify before continuing.
  - **Safety or approval** (e.g. destructive change, external side effects, compliance) — describe the action and ask for explicit approval.
  - **Repeated direct-execution failure** — if you attempted the task yourself and still cannot complete it (e.g. blocked by environment or complexity), note the task and escalate with a short summary and suggested next step.

## Pattern 1: Parallel batch execution (primary) — MANDATORY for execution

When executing a plan with multiple unblocked tasks, you MUST use this pattern. **Feed all non-conflicting tasks; Cursor decides how many run in parallel.**

0. **TodoWrite**: Once you have the runnable batch (from steps 1–2), call TodoWrite with it (see Task orchestration UI — TodoWrite protocol above). Map each task to a todo item (id, content, status); use merge=false before any dispatch. For a single-task batch, add a second item (e.g. "Review & gate") so the list has at least 2 entries.
1. Run `pnpm tg next --plan "<Plan>" --json --limit 20` (or `tg next --json --limit 20` for multi-plan) to get runnable tasks.
2. **File conflict check**: From the list, keep only tasks that do not touch the same files (compare `file_tree`, `suggested_changes`, or intent). Do not cap the batch size — include every runnable task that is independent of the others. Cursor will decide how many sub-agents run concurrently.
3. For each task in the batch, run `pnpm tg context <taskId> --json` and capture the JSON.
4. Inspect the `agent` field from the context JSON: if set, select the corresponding agent template (`.cursor/agents/{{AGENT}}.md`); otherwise default to `.cursor/agents/implementer.md`. For tasks with agent debugger, use `.cursor/agents/debugger.md`; ensure failure or escalation note from the debugger is handled by Follow-up from notes/evidence (orchestrator creates investigate task or escalates). Build the sub-agent prompt by replacing placeholders (`{{TASK_ID}}`, `{{AGENT_NAME}}`, `{{TITLE}}`, `{{INTENT}}`, `{{DOC_PATHS}}`, `{{SKILL_DOCS}}`, `{{SUGGESTED_CHANGES}}`, `{{FILE_TREE}}`, `{{RISKS}}`, `{{RELATED_DONE}}`, `{{EXPLORER_OUTPUT}}`) with values from the context JSON. Refer to the agent registry rule at `.cursor/rules/available-agents.mdc` for valid agent names. Use unique agent names (e.g. implementer-1, implementer-2, …) so `tg status` shows distinct workers. **Worktrees for parallel tasks:** Use Worktrunk for isolation (see "Worktrunk — standard for sub-agent worktrees" above). In the prompt, instruct the implementer to: (1) run `pnpm tg start <taskId> --agent <name> --worktree` from the repo root; (2) obtain the worktree path (from your inject of **{{WORKTREE_PATH}}** if you run start yourself and pass it, or from `tg worktree list --json` after start); (3) `cd` to that path before doing any implementation work. Pass **{{WORKTREE_PATH}}** explicitly in the implementer prompt when you have it (e.g. from a prior `tg start ... --worktree` plus `tg worktree list --json`, or from the started event). With Worktrunk, paths are wt-managed (e.g. `<repo>.tg-<hash>`); with raw git, `<repo>/.taskgraph/worktrees/<taskId>`. All work and `tg done` run from that worktree directory.
5. **Dispatch** all tasks in the batch **in the same response** (one Task or mcp_task call per task in the batch). Do not dispatch one task per turn — emit all N calls in the same message so Cursor runs them in parallel and surfaces the orchestration UI. Same built prompt and a short description (e.g. "Implement task: <title>") per call. Cursor decides concurrency; do not artificially limit the number.
6. Wait for all to complete. For each completed task, if you have the implementer's diff, run the **two-stage review**: (1) dispatch **spec-reviewer** (read `.cursor/agents/spec-reviewer.md`, build prompt with task context + diff; Task tool, agent CLI, or mcp_task). **After spec-reviewer completes**, record the verdict: `tg note <taskId> --msg '{"type":"review","verdict":"PASS"|"FAIL","reviewer":"<reviewer_agent_name>","stage":"spec"}'` (use the actual verdict and reviewer name). (2) If spec-reviewer returns PASS, dispatch **quality-reviewer** (read `.cursor/agents/quality-reviewer.md`, build prompt with task context + diff; same mechanism). **After quality-reviewer completes**, record the verdict: `tg note <taskId> --msg '{"type":"review","verdict":"PASS"|"FAIL","reviewer":"<reviewer_agent_name>","stage":"quality"}'`. (3) If either returns FAIL, re-dispatch the implementer with that reviewer's specific feedback. After 2 re-dispatch failures for that task, do that task yourself (direct execution).
7. **Follow-up from notes/evidence**: For each completed task, read the implementer's return message and any notes on the task (e.g. `tg show <taskId>` for recent events). If the sub-agent reported environment limitations, gate failures, or suggested follow-up, apply the **Follow-up from notes/evidence** protocol below.
8. Run `pnpm tg next --plan "<Plan>" --json --limit 20` again. If more runnable tasks exist, go to step 2. Otherwise continue with any remaining work or finish the plan.

## Pattern 2: Sequential single-task execution — MANDATORY when only one runnable

When only one task is runnable or tasks share files (so you cannot batch), you MUST still use a sub-agent:

0. **TodoWrite**: Call TodoWrite with the task list (see Task orchestration UI — TodoWrite protocol above). For a single task, use 2 items (the task plus e.g. "Review & gate") so the list has at least 2 entries; update statuses with merge=true as the task completes.
1. Run `pnpm tg next --plan "<Plan>" --limit 1` and pick the task.
2. Optionally dispatch the **explorer** (read `.cursor/agents/explorer.md`, build prompt; use Task tool, agent CLI, or mcp_task). Use explorer output to enrich implementer context. Skip explorer for very simple tasks (e.g. add a test file, document X).
3. Run `pnpm tg context <taskId> --json`. Build the implementer prompt from the template and context (and explorer output if present). **Dispatch** one implementer (Task tool, agent CLI, or mcp_task) with the built prompt and description "Implement: <title>".
4. When implementer completes, run the **two-stage review**: (1) dispatch **spec-reviewer** (read `.cursor/agents/spec-reviewer.md`, build prompt with task context + diff; Task tool, agent CLI, or mcp_task). **After spec-reviewer completes**, record the verdict: `tg note <taskId> --msg '{"type":"review","verdict":"PASS"|"FAIL","reviewer":"<reviewer_agent_name>","stage":"spec"}'`. (2) If spec-reviewer returns PASS, dispatch **quality-reviewer** (read `.cursor/agents/quality-reviewer.md`, build prompt with task context + diff; same mechanism). **After quality-reviewer completes**, record the verdict: `tg note <taskId> --msg '{"type":"review","verdict":"PASS"|"FAIL","reviewer":"<reviewer_agent_name>","stage":"quality"}'`. (3) If either returns FAIL, re-dispatch the implementer with that reviewer's specific feedback. After 2 re-dispatch failures, complete the task yourself.
5. **Follow-up from notes/evidence**: Read the implementer's return message and any notes on the task (`tg show <taskId>`). If they reported environment limitations, gate failures, or suggested follow-up, apply the **Follow-up from notes/evidence** protocol below.
6. Repeat from step 1 for the next task.

## Follow-up from notes/evidence (orchestrator decides)

When an implementer (or other sub-agent) completes and their return message, evidence, or task notes mention environment limitations (e.g. "bun not available"), gate failures (e.g. "typecheck failed"), or suggested follow-up (e.g. "fix bun-types in tsconfig"), the **orchestrator** must decide whether to investigate further.

1. **Evaluate**: Does the finding warrant a follow-up task? Consider: Is it blocking the plan or other work? A known limitation of the sub-agent environment that the human can ignore? Out of scope for the current plan?
2. **If follow-up is warranted**: Create one or more tasks with `tg task new "<title>" --plan <planId>` (use the plan of the task that just completed). Optionally record the link: `tg note <completedTaskId> --msg "Follow-up: <newTaskId> <brief reason>"`. Delegate the new task(s) via the normal dispatch flow (they will appear in the next `tg next` batch, or dispatch them in the same batch if you have capacity).
3. **If not warranted**: Continue without creating tasks. Optionally `tg note <completedTaskId> --msg "Orchestrator: no follow-up; <brief reason>"` for audit.

Implementers should leave a `tg note` when they hit environment or gate issues they cannot fix, so the orchestrator can evaluate and optionally spawn follow-up tasks.

**Full-suite (gate:full) failure:** When the task is "run full test suite" (or equivalent) and `pnpm gate:full` fails, the agent **marks the task done** with evidence e.g. `gate:full failed: <brief reason>` and adds a `tg note` with the failure reason. The lead sees the outcome in `tg show` / evidence and uses the Follow-up from notes/evidence protocol above to create fix tasks. Do not leave the task not-done; using done + note keeps status clean and reuses the same mechanism as other gate/environment follow-up.

## Pattern 3: Plan analysis (before writing a plan) — MANDATORY

**You must use this pattern whenever the user asks for a plan.** Do not write a plan without first dispatching the planner-analyst.

Before creating a new plan (e.g. user asked for a feature and you are about to write `plans/yy-mm-dd_<name>.md`):

1. Read `.cursor/agents/planner-analyst.md`. Build the prompt with `{{REQUEST}}` = user's feature/request. Optionally run `pnpm tg status --tasks` and pass the output so the analyst can reference current task state (full list, not limited to 3).
2. **Dispatch** the planner-analyst (Task tool, agent CLI, or mcp_task with subagent_type explore) with the built prompt and description "Planner analyst: gather context for plan".
3. Use the analyst's output as input when you write the plan. **You own architecture, dependencies, and task design** — the analyst gathers facts, you do the reasoning.

### Orchestrator critique checklist (after receiving analyst output)

Do not transcribe the analyst's suggested breakdown into tasks verbatim. Before writing the plan, work through this checklist:

- **Existing data first**: What metrics or insights can be derived from data already in the system (timestamps, event counts, existing fields) before designing new data capture? The analyst should have surfaced this; if not, identify it yourself.
- **Dependency minimization**: For each proposed dependency between tasks, ask: "Can the downstream task work (even partially) without the upstream?" Decouple data-capture features from features that can function without that data. Prefer a wide graph over a deep chain.
- **Concrete metrics**: When the user asks for something qualitative ("efficiency", "performance", "improvements"), define it in measurable terms in the plan. What exactly gets measured? What thresholds or comparisons make it actionable? If the analyst left this vague, sharpen it.
- **Task specificity**: Each task must be concrete enough that a fast sub-agent can implement it without making design decisions. If intent says "heuristics e.g. ..." or "optionally ...", the task needs more design work from you before it's ready.
- **Resolve open questions**: Architectural choices (e.g. "same command or separate?") should be decided in the plan, not left as open questions for implementers. If you genuinely can't decide, create an explicit investigate task.
- **Test coverage**: Either include test expectations in each task's intent, or create a dedicated test task. Don't list tests in plan-level `tests` without assigning them to a task.
- **AI-slop patterns**: Flag and reject: scope inflation ("also add tests for adjacent modules"), premature abstraction ("extract to shared utility"), over-validation ("15 error checks for 3 inputs"). If the analyst's suggestions show these patterns, trim them before writing the plan.

## Learning Mode

**Toggle**: Check `"learningMode"` in `.taskgraph/config.json`. If `true`, learning mode is ON — run the review protocol below after each sub-agent completes. If `false` or absent, skip this section entirely.

### When it triggers

After every **implementer**, **explorer**, or **planner-analyst** sub-agent completes. Not after the reviewer (its job is already evaluation).

### Review protocol (orchestrator only)

When learning mode is ON and a sub-agent has completed:

1. **Gather the sub-agent's output**: the returned message, and for implementers also the git diff (`git diff HEAD~1` or similar).
2. **Evaluate**: Would you (the orchestrator) have done the same thing? Consider:
   - **Scope**: Did the sub-agent stay within the task's intent, or drift?
   - **Approach**: Was the implementation approach what you'd have chosen? If not, what's better and why?
   - **Quality**: Missing error handling, unnecessary complexity, wrong abstractions, missed patterns from the codebase?
   - **Context use**: Did the sub-agent make good use of the domain docs, skill guides, and suggested changes it was given? Or did it ignore them?
3. **Decide**: Is there a constructive, reusable learning here? Skip if the sub-agent did fine — only write learnings when there's a concrete improvement to encode. Not every task produces a learning.
4. **Write the learning**: Append to the `## Learnings` section of the relevant agent file (e.g. `.cursor/agents/implementer.md`). Use this format:

```
- **[YYYY-MM-DD]** <one-line summary>. <concrete directive: "Instead, do X" or "Always check Y before Z".>
```

5. **Consolidate** (periodically): When `## Learnings` exceeds ~10 entries, fold recurring patterns into the agent's main prompt template and remove the individual entries. This keeps the learnings section high-signal and prevents prompt bloat.

### Injecting learnings into prompts

When building a sub-agent prompt, if the agent file has a non-empty `## Learnings` section, include it in the prompt as a `{{LEARNINGS}}` block:

```
**Learnings from prior runs (follow these):**
{{LEARNINGS}}
```

Place this after the main instructions and before any task-specific context. The sub-agent should treat learnings as corrections to its default behavior.

## Lifecycle and errors

- Every implementer sub-agent must run `pnpm tg start <taskId> --agent <name> --worktree` at the start (when using worktree isolation) and `pnpm tg done <taskId> --evidence "..."` at the end. Do not skip — the task graph is the source of truth. Use Worktrunk when available (config `useWorktrunk: true` or wt on PATH). In Pattern 1 (parallel batch), pass the worktree path ({{WORKTREE_PATH}}) so the implementer runs from that directory.
- If a fast sub-agent fails (error, wrong result, or spec-reviewer/quality-reviewer FAIL): re-dispatch once with clearer context or that reviewer's feedback. If it fails again, complete that task yourself with direct execution (session model).
- When dispatching in parallel, ensure each task has a distinct agent name (`implementer-1`, etc.) so `tg status` and events stay clear.

## Building prompts from context JSON

`pnpm tg context <taskId> --json` returns an object with: `task_id`, `title`, `agent`, `docs`, `skills`, `change_type`, `suggested_changes`, `file_tree`, `risks`, `doc_paths`, `skill_docs`, `related_done_by_domain`, `related_done_by_skill`. Map these to the placeholders in each agent template (see `.cursor/agents/README.md`). For **implementer** when using worktrees (Pattern 1): also pass **WORKTREE_PATH** — run `tg start <taskId> --agent <name> --worktree` from repo root, then `tg worktree list --json` and use the path for the branch matching this task (e.g. `tg-<hash_id>`), or read from the started event; inject as `{{WORKTREE_PATH}}` in the implementer prompt. For the spec-reviewer and quality-reviewer you also need the implementer's git diff (e.g. `git diff` or `git show` after the implementer's run).

Additionally, for every agent: read the agent file's `## Learnings` section (if non-empty) and inject it as `{{LEARNINGS}}`. See Learning Mode above.
